{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5137200,"sourceType":"datasetVersion","datasetId":2984453},{"sourceId":12273644,"sourceType":"datasetVersion","datasetId":7734571},{"sourceId":12624574,"sourceType":"datasetVersion","datasetId":7787772}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip show tensorflow tensorflow-probability\n!pip install tensorflow-probability==0.24.0\n!pip install --upgrade keras-tuner\n!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.databases #Package for database reading.\n!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.visualizations.git\n!pip install mne #The MNE Package is installed\nFILEID = \"1lo0MjWLvsyne2CgTA6VZ2HGY9SKxiwZ7\"\n!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"$FILEID -O MI_EEG_ClassMeth.zip && rm -rf /tmp/cookies.txt\n!unzip MI_EEG_ClassMeth.zip -y #Package with useful functions for motor imagery classification based in EEG.\n!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git\n!dir","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-08-01T13:23:05.429710Z","iopub.execute_input":"2025-08-01T13:23:05.430237Z","iopub.status.idle":"2025-08-01T13:23:56.960639Z","shell.execute_reply.started":"2025-08-01T13:23:05.430215Z","shell.execute_reply":"2025-08-01T13:23:56.959921Z"}},"outputs":[{"name":"stdout","text":"Name: tensorflow\nVersion: 2.18.0\nSummary: TensorFlow is an open source machine learning framework for everyone.\nHome-page: https://www.tensorflow.org/\nAuthor: Google Inc.\nAuthor-email: packages@tensorflow.org\nLicense: Apache 2.0\nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\nRequired-by: dopamine_rl, tensorflow-text, tensorflow_decision_forests, tf_keras\n---\nName: tensorflow-probability\nVersion: 0.25.0\nSummary: Probabilistic modeling and statistical inference in TensorFlow\nHome-page: http://github.com/tensorflow/probability\nAuthor: Google LLC\nAuthor-email: no-reply@google.com\nLicense: Apache 2.0\nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: absl-py, cloudpickle, decorator, dm-tree, gast, numpy, six\nRequired-by: dopamine_rl\nCollecting tensorflow-probability==0.24.0\n  Downloading tensorflow_probability-0.24.0-py2.py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (1.4.0)\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (1.17.0)\nRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (1.26.4)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (4.4.2)\nRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (3.1.1)\nRequirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (0.6.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (0.1.9)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.3->tensorflow-probability==0.24.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.3->tensorflow-probability==0.24.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.3->tensorflow-probability==0.24.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.3->tensorflow-probability==0.24.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.3->tensorflow-probability==0.24.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.3->tensorflow-probability==0.24.0) (2.4.1)\nRequirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability==0.24.0) (25.3.0)\nRequirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability==0.24.0) (1.17.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.13.3->tensorflow-probability==0.24.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.13.3->tensorflow-probability==0.24.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.13.3->tensorflow-probability==0.24.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.13.3->tensorflow-probability==0.24.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.13.3->tensorflow-probability==0.24.0) (2024.2.0)\nDownloading tensorflow_probability-0.24.0-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tensorflow-probability\n  Attempting uninstall: tensorflow-probability\n    Found existing installation: tensorflow-probability 0.25.0\n    Uninstalling tensorflow-probability-0.25.0:\n      Successfully uninstalled tensorflow-probability-0.25.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tensorflow-probability-0.24.0\nRequirement already satisfied: keras-tuner in /usr/local/lib/python3.11/dist-packages (1.4.7)\nRequirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\nRequirement already satisfied: kt-legacy in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (1.0.5)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.26.4)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.13.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.4.26)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->keras->keras-tuner) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->keras->keras-tuner) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->keras->keras-tuner) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->keras->keras-tuner) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->keras->keras-tuner) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->keras->keras-tuner) (2.4.1)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.13.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras->keras-tuner) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras->keras-tuner) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->keras->keras-tuner) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->keras->keras-tuner) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->keras->keras-tuner) (2024.2.0)\nCollecting git+https://github.com/UN-GCPDS/python-gcpds.databases\n  Cloning https://github.com/UN-GCPDS/python-gcpds.databases to /tmp/pip-req-build-im94vagc\n  Running command git clone --filter=blob:none --quiet https://github.com/UN-GCPDS/python-gcpds.databases /tmp/pip-req-build-im94vagc\n  Resolved https://github.com/UN-GCPDS/python-gcpds.databases to commit d174df9958b6638156dcfe03996a6307e631a6a2\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from gcpds-databases==0.2) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gcpds-databases==0.2) (1.15.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from gcpds-databases==0.2) (3.7.2)\nRequirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (from gcpds-databases==0.2) (1.9.0)\nRequirement already satisfied: tables in /usr/local/lib/python3.11/dist-packages (from gcpds-databases==0.2) (3.10.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gcpds-databases==0.2) (4.67.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from gcpds-databases==0.2) (2.2.3)\nRequirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (from gcpds-databases==0.2) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown->gcpds-databases==0.2) (4.13.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown->gcpds-databases==0.2) (3.18.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown->gcpds-databases==0.2) (2.32.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gcpds-databases==0.2) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gcpds-databases==0.2) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gcpds-databases==0.2) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gcpds-databases==0.2) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gcpds-databases==0.2) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gcpds-databases==0.2) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gcpds-databases==0.2) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->gcpds-databases==0.2) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->gcpds-databases==0.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->gcpds-databases==0.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->gcpds-databases==0.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->gcpds-databases==0.2) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->gcpds-databases==0.2) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->gcpds-databases==0.2) (2.4.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne->gcpds-databases==0.2) (4.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne->gcpds-databases==0.2) (3.1.6)\nRequirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne->gcpds-databases==0.2) (0.4)\nRequirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne->gcpds-databases==0.2) (1.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->gcpds-databases==0.2) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->gcpds-databases==0.2) (2025.2)\nRequirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.11/dist-packages (from tables->gcpds-databases==0.2) (2.10.2)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from tables->gcpds-databases==0.2) (9.0.0)\nRequirement already satisfied: blosc2>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from tables->gcpds-databases==0.2) (3.2.1)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from tables->gcpds-databases==0.2) (4.13.2)\nRequirement already satisfied: ndindex in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables->gcpds-databases==0.2) (1.9.2)\nRequirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables->gcpds-databases==0.2) (1.1.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from blosc2>=2.3.0->tables->gcpds-databases==0.2) (4.3.8)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->gcpds-databases==0.2) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown->gcpds-databases==0.2) (2.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne->gcpds-databases==0.2) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->gcpds-databases==0.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->gcpds-databases==0.2) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->gcpds-databases==0.2) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->gcpds-databases==0.2) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->gcpds-databases==0.2) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->gcpds-databases==0.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->gcpds-databases==0.2) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->gcpds-databases==0.2) (2025.4.26)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->gcpds-databases==0.2) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->gcpds-databases==0.2) (2024.2.0)\nBuilding wheels for collected packages: gcpds-databases\n  Building wheel for gcpds-databases (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for gcpds-databases: filename=gcpds_databases-0.2-py3-none-any.whl size=32972809 sha256=0f6663d0999814255d6f1f8cc30bc87516d3f017be45910da865bc20e5dee610\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ahk_n3un/wheels/ae/48/8d/edf617d5fe8f03b17aa26306a04abdfcc605b218d8e6deac83\nSuccessfully built gcpds-databases\nInstalling collected packages: gcpds-databases\nSuccessfully installed gcpds-databases-0.2\nCollecting git+https://github.com/UN-GCPDS/python-gcpds.visualizations.git\n  Cloning https://github.com/UN-GCPDS/python-gcpds.visualizations.git to /tmp/pip-req-build-cbn3dmbq\n  Running command git clone --filter=blob:none --quiet https://github.com/UN-GCPDS/python-gcpds.visualizations.git /tmp/pip-req-build-cbn3dmbq\n  Resolved https://github.com/UN-GCPDS/python-gcpds.visualizations.git to commit 162dbeac141a7472d3b0bd7f005932241b4663a5\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting python-circos (from gcpds-visualizations==0.6)\n  Downloading python_circos-0.3.0-py3-none-any.whl.metadata (766 bytes)\nRequirement already satisfied: matplotlib>=3.5.3 in /usr/local/lib/python3.11/dist-packages (from gcpds-visualizations==0.6) (3.7.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from gcpds-visualizations==0.6) (1.26.4)\nRequirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (from gcpds-visualizations==0.6) (1.9.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->gcpds-visualizations==0.6) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->gcpds-visualizations==0.6) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->gcpds-visualizations==0.6) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->gcpds-visualizations==0.6) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->gcpds-visualizations==0.6) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->gcpds-visualizations==0.6) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->gcpds-visualizations==0.6) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.3->gcpds-visualizations==0.6) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->gcpds-visualizations==0.6) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->gcpds-visualizations==0.6) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->gcpds-visualizations==0.6) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->gcpds-visualizations==0.6) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->gcpds-visualizations==0.6) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->gcpds-visualizations==0.6) (2.4.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne->gcpds-visualizations==0.6) (4.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne->gcpds-visualizations==0.6) (3.1.6)\nRequirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne->gcpds-visualizations==0.6) (0.4)\nRequirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne->gcpds-visualizations==0.6) (1.8.2)\nRequirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne->gcpds-visualizations==0.6) (1.15.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne->gcpds-visualizations==0.6) (4.67.1)\nCollecting biopython>=1.78 (from python-circos->gcpds-visualizations==0.6)\n  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne->gcpds-visualizations==0.6) (4.3.8)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne->gcpds-visualizations==0.6) (2.32.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.3->gcpds-visualizations==0.6) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne->gcpds-visualizations==0.6) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->gcpds-visualizations==0.6) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->gcpds-visualizations==0.6) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->gcpds-visualizations==0.6) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->gcpds-visualizations==0.6) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->gcpds-visualizations==0.6) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->gcpds-visualizations==0.6) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->gcpds-visualizations==0.6) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->gcpds-visualizations==0.6) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->gcpds-visualizations==0.6) (2025.4.26)\nDownloading python_circos-0.3.0-py3-none-any.whl (27 kB)\nDownloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: gcpds-visualizations\n  Building wheel for gcpds-visualizations (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for gcpds-visualizations: filename=gcpds_visualizations-0.6-py3-none-any.whl size=12440 sha256=6a0c9819a48b8d3e3a1a33def0f52cc20986276253726914ac134c9c6b5ec2c7\n  Stored in directory: /tmp/pip-ephem-wheel-cache-yixi6ju5/wheels/12/e9/11/f7246de13d1d668c154d68fb1260be82dbfbc166301807d756\nSuccessfully built gcpds-visualizations\nInstalling collected packages: biopython, python-circos, gcpds-visualizations\nSuccessfully installed biopython-1.85 gcpds-visualizations-0.6 python-circos-0.3.0\nRequirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (1.9.0)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\nRequirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\nRequirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.7.2)\nRequirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (25.0)\nRequirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\nRequirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.15.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->mne) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->mne) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->mne) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->mne) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->mne) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.23->mne) (2.4.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.8)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->mne) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.23->mne) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->mne) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.23->mne) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.23->mne) (2024.2.0)\n--2025-08-01 13:23:44--  https://docs.google.com/uc?export=download&confirm=&id=1lo0MjWLvsyne2CgTA6VZ2HGY9SKxiwZ7\nResolving docs.google.com (docs.google.com)... 209.85.145.102, 209.85.145.139, 209.85.145.138, ...\nConnecting to docs.google.com (docs.google.com)|209.85.145.102|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://drive.usercontent.google.com/download?id=1lo0MjWLvsyne2CgTA6VZ2HGY9SKxiwZ7&export=download [following]\n--2025-08-01 13:23:45--  https://drive.usercontent.google.com/download?id=1lo0MjWLvsyne2CgTA6VZ2HGY9SKxiwZ7&export=download\nResolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.132.132, 2607:f8b0:4001:c00::84\nConnecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.132.132|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 86900 (85K) [application/octet-stream]\nSaving to: ‘MI_EEG_ClassMeth.zip’\n\nMI_EEG_ClassMeth.zi 100%[===================>]  84.86K  --.-KB/s    in 0.001s  \n\n2025-08-01 13:23:46 (79.0 MB/s) - ‘MI_EEG_ClassMeth.zip’ saved [86900/86900]\n\nArchive:  MI_EEG_ClassMeth.zip\ncaution: filename not matched:  -y\nCollecting git+https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git\n  Cloning https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git to /tmp/pip-req-build-um8unsmf\n  Running command git clone --filter=blob:none --quiet https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git /tmp/pip-req-build-um8unsmf\n  Resolved https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git to commit 0c791f236d503dac4829adb78cdba759c5843417\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting braindecode==0.7 (from EEG_Tensorflow_models==0.2)\n  Downloading Braindecode-0.7-py3-none-any.whl.metadata (6.8 kB)\nCollecting moabb (from EEG_Tensorflow_models==0.2)\n  Downloading moabb-1.2.0-py3-none-any.whl.metadata (14 kB)\nCollecting tensorflow-addons (from EEG_Tensorflow_models==0.2)\n  Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: tensorflow>=2.8 in /usr/local/lib/python3.11/dist-packages (from EEG_Tensorflow_models==0.2) (2.18.0)\nCollecting tf-keras-vis (from EEG_Tensorflow_models==0.2)\n  Downloading tf_keras_vis-0.8.7-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (from braindecode==0.7->EEG_Tensorflow_models==0.2) (1.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from braindecode==0.7->EEG_Tensorflow_models==0.2) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from braindecode==0.7->EEG_Tensorflow_models==0.2) (2.2.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from braindecode==0.7->EEG_Tensorflow_models==0.2) (1.15.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from braindecode==0.7->EEG_Tensorflow_models==0.2) (3.7.2)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from braindecode==0.7->EEG_Tensorflow_models==0.2) (3.13.0)\nCollecting skorch (from braindecode==0.7->EEG_Tensorflow_models==0.2)\n  Downloading skorch-1.1.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (3.0.1)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (4.13.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (1.72.0rc1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (3.8.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.8->EEG_Tensorflow_models==0.2) (0.37.1)\nRequirement already satisfied: PyYAML<7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from moabb->EEG_Tensorflow_models==0.2) (6.0.2)\nRequirement already satisfied: coverage<8.0.0,>=7.0.1 in /usr/local/lib/python3.11/dist-packages (from moabb->EEG_Tensorflow_models==0.2) (7.8.0)\nCollecting edfio<0.5.0,>=0.4.2 (from moabb->EEG_Tensorflow_models==0.2)\n  Downloading edfio-0.4.9-py3-none-any.whl.metadata (3.9 kB)\nCollecting edflib-python<2.0.0,>=1.0.6 (from moabb->EEG_Tensorflow_models==0.2)\n  Downloading EDFlib_Python-1.0.8-py3-none-any.whl.metadata (1.3 kB)\nCollecting memory-profiler<0.62.0,>=0.61.0 (from moabb->EEG_Tensorflow_models==0.2)\n  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\nCollecting mne-bids>=0.14 (from moabb->EEG_Tensorflow_models==0.2)\n  Downloading mne_bids-0.16.0-py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: pooch<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from moabb->EEG_Tensorflow_models==0.2) (1.8.2)\nCollecting pyriemann<0.8,>=0.7 (from moabb->EEG_Tensorflow_models==0.2)\n  Downloading pyriemann-0.7-py2.py3-none-any.whl.metadata (9.1 kB)\nRequirement already satisfied: scikit-learn<1.6 in /usr/local/lib/python3.11/dist-packages (from moabb->EEG_Tensorflow_models==0.2) (1.2.2)\nRequirement already satisfied: seaborn<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from moabb->EEG_Tensorflow_models==0.2) (0.12.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from moabb->EEG_Tensorflow_models==0.2) (4.67.1)\nCollecting urllib3<2.0.0,>=1.26.15 (from moabb->EEG_Tensorflow_models==0.2)\n  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->EEG_Tensorflow_models==0.2)\n  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from tf-keras-vis->EEG_Tensorflow_models==0.2) (11.1.0)\nRequirement already satisfied: deprecated in /usr/local/lib/python3.11/dist-packages (from tf-keras-vis->EEG_Tensorflow_models==0.2) (1.2.18)\nRequirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from tf-keras-vis->EEG_Tensorflow_models==0.2) (2.37.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (0.14.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode==0.7->EEG_Tensorflow_models==0.2) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode==0.7->EEG_Tensorflow_models==0.2) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode==0.7->EEG_Tensorflow_models==0.2) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode==0.7->EEG_Tensorflow_models==0.2) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode==0.7->EEG_Tensorflow_models==0.2) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode==0.7->EEG_Tensorflow_models==0.2) (2.9.0.post0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from memory-profiler<0.62.0,>=0.61.0->moabb->EEG_Tensorflow_models==0.2) (7.0.0)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne->braindecode==0.7->EEG_Tensorflow_models==0.2) (4.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne->braindecode==0.7->EEG_Tensorflow_models==0.2) (3.1.6)\nRequirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne->braindecode==0.7->EEG_Tensorflow_models==0.2) (0.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->braindecode==0.7->EEG_Tensorflow_models==0.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->braindecode==0.7->EEG_Tensorflow_models==0.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->braindecode==0.7->EEG_Tensorflow_models==0.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->braindecode==0.7->EEG_Tensorflow_models==0.2) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->braindecode==0.7->EEG_Tensorflow_models==0.2) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->braindecode==0.7->EEG_Tensorflow_models==0.2) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->braindecode==0.7->EEG_Tensorflow_models==0.2) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->braindecode==0.7->EEG_Tensorflow_models==0.2) (2025.2)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch<2.0.0,>=1.6.0->moabb->EEG_Tensorflow_models==0.2) (4.3.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from pyriemann<0.8,>=0.7->moabb->EEG_Tensorflow_models==0.2) (1.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (2025.4.26)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6->moabb->EEG_Tensorflow_models==0.2) (3.6.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (3.1.3)\nRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from skorch->braindecode==0.7->EEG_Tensorflow_models==0.2) (0.9.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->braindecode==0.7->EEG_Tensorflow_models==0.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->braindecode==0.7->EEG_Tensorflow_models==0.2) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->braindecode==0.7->EEG_Tensorflow_models==0.2) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->braindecode==0.7->EEG_Tensorflow_models==0.2) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->braindecode==0.7->EEG_Tensorflow_models==0.2) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.8->EEG_Tensorflow_models==0.2) (0.1.2)\nDownloading Braindecode-0.7-py3-none-any.whl (184 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/184.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading moabb-1.2.0-py3-none-any.whl (242 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.3/242.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tf_keras_vis-0.8.7-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading edfio-0.4.9-py3-none-any.whl (27 kB)\nDownloading EDFlib_Python-1.0.8-py3-none-any.whl (26 kB)\nDownloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\nDownloading mne_bids-0.16.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.8/166.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyriemann-0.7-py2.py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.6/115.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\nDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading skorch-1.1.0-py3-none-any.whl (228 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.9/228.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: EEG_Tensorflow_models\n  Building wheel for EEG_Tensorflow_models (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for EEG_Tensorflow_models: filename=EEG_Tensorflow_models-0.2-py3-none-any.whl size=29287 sha256=b11fdbb434cb60317427a924dbba6eed5392ee2ef47f7217499dee71a74314e2\n  Stored in directory: /tmp/pip-ephem-wheel-cache-3sl6967p/wheels/ec/2b/bd/488f6c2631523174d34618ee5e61f72194b1389c81838cfd71\nSuccessfully built EEG_Tensorflow_models\nInstalling collected packages: urllib3, typeguard, memory-profiler, tensorflow-addons, skorch, pyriemann, mne-bids, edflib-python, edfio, tf-keras-vis, moabb, braindecode, EEG_Tensorflow_models\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.4.0\n    Uninstalling urllib3-2.4.0:\n      Successfully uninstalled urllib3-2.4.0\n  Attempting uninstall: typeguard\n    Found existing installation: typeguard 4.4.2\n    Uninstalling typeguard-4.4.2:\n      Successfully uninstalled typeguard-4.4.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nydata-profiling 4.16.1 requires typeguard<5,>=3, but you have typeguard 2.13.3 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\ninflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed EEG_Tensorflow_models-0.2 braindecode-0.7 edfio-0.4.9 edflib-python-1.0.8 memory-profiler-0.61.0 mne-bids-0.16.0 moabb-1.2.0 pyriemann-0.7 skorch-1.1.0 tensorflow-addons-0.23.0 tf-keras-vis-0.8.7 typeguard-2.13.3 urllib3-1.26.20\nMI_EEG_ClassMeth.zip\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade botocore tensorboard","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:24:03.461171Z","iopub.execute_input":"2025-08-01T13:24:03.461441Z","iopub.status.idle":"2025-08-01T13:24:14.143294Z","shell.execute_reply.started":"2025-08-01T13:24:03.461418Z","shell.execute_reply":"2025-08-01T13:24:14.142592Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: botocore in /usr/local/lib/python3.11/dist-packages (1.38.11)\nCollecting botocore\n  Downloading botocore-1.40.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\nCollecting tensorboard\n  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from botocore) (1.0.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore) (2.9.0.post0)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore) (1.26.20)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.72.0rc1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\nRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (25.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from tensorboard) (11.1.0)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.12.0->tensorboard) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.12.0->tensorboard) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.12.0->tensorboard) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.12.0->tensorboard) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.12.0->tensorboard) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.12.0->tensorboard) (2024.2.0)\nDownloading botocore-1.40.0-py3-none-any.whl (13.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: botocore, tensorboard\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.38.11\n    Uninstalling botocore-1.38.11:\n      Successfully uninstalled botocore-1.38.11\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.18.0\n    Uninstalling tensorboard-2.18.0:\n      Successfully uninstalled tensorboard-2.18.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nboto3 1.38.11 requires botocore<1.39.0,>=1.38.11, but you have botocore 1.40.0 which is incompatible.\ntensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed botocore-1.40.0 tensorboard-2.20.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport os\nimport itertools\nimport random\nimport pickle\nimport mne\nimport h5py\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport matplotlib.pyplot as plt\nimport keras_tuner as kt\n\n\nfrom gcpds.databases.BCI_Competition_IV import Dataset_2a\nfrom typing import Sequence, Tuple\nfrom scipy.signal import iirnotch, filtfilt, butter, freqz\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance\nimport networkx as nx\nfrom tqdm import tqdm\nfrom mne.preprocessing import compute_current_source_density\nfrom mne.channels import make_standard_montage, read_custom_montage\nfrom scipy.signal import butter, filtfilt, resample, iirnotch\nfrom gcpds.visualizations.series import plot_eeg\nfrom scipy.stats import norm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score\nfrom tqdm import tqdm\n\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nfrom gcpds.databases import GIGA_MI_ME\nfrom sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom tensorflow.keras.utils import register_keras_serializable\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import LeavePGroupsOut, StratifiedGroupKFold\nfrom tensorflow.keras.models import Model\nfrom scipy.spatial.distance import cdist\nfrom sklearn.model_selection import GroupKFold\nfrom tensorflow.keras.models import load_model\n\nfrom keras_tuner import Objective\nfrom keras_tuner import HyperModel\nfrom keras.layers import Layer, Activation\nfrom keras_tuner import BayesianOptimization\nfrom keras_tuner.engine.hyperparameters import HyperParameters\nfrom tensorflow.keras.metrics import BinaryAccuracy, Recall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:24:44.656750Z","iopub.execute_input":"2025-08-01T13:24:44.657099Z","iopub.status.idle":"2025-08-01T13:24:44.668201Z","shell.execute_reply.started":"2025-08-01T13:24:44.657077Z","shell.execute_reply":"2025-08-01T13:24:44.667344Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_BCICIV2a(db, sbj: int, mode: str, fs: float) -> tuple:\n    \"\"\"\n    Carga los datos EEG para un sujeto específico con preprocesamiento, incluyendo filtrado y laplaciano superficial.\n    Se recorta el tiempo entre los segundos 2 y 4.\n    \n    Args:\n        db (Dataset_2a): Objeto del dataset.\n        sbj (int): Identificador del sujeto (1-9).\n        mode (str): 'training' o 'testing'.\n        fs (float): Frecuencia de muestreo.\n\n    Returns:\n        tuple: Datos EEG preprocesados con laplaciano superficial y recortados en el tiempo (X), etiquetas (y).\n    \"\"\"\n    # Cargar los datos del sujeto\n    db.load_subject(sbj, mode=mode)\n    X, y = db.get_data()  # Datos y etiquetas\n    X = X[:, :-3, :]  # Seleccionar solo los canales EEG (22 canales)\n    X = X * 1e6  # Convertir a microvoltios\n\n    # Aplicar filtro Notch (50 Hz)\n    notch_freq = 50.0\n    q_factor = 30.0\n    b_notch, a_notch = iirnotch(w0=notch_freq, Q=q_factor, fs=fs)\n    X = filtfilt(b_notch, a_notch, X, axis=2)\n\n    # Aplicar filtro pasa banda (0.5 - 100 Hz)\n    lowcut = 0.5\n    highcut = 100.0\n    b_band, a_band = butter(N=4, Wn=[lowcut, highcut], btype='bandpass', fs=fs)\n    X = filtfilt(b_band, a_band, X, axis=2)\n\n    # Recortar los datos entre el segundo 2 y el segundo 4\n    start_sample = int(2 * fs)  # Inicio en el segundo 2\n    end_sample = int(4 * fs)    # Fin en el segundo 4\n    X = X[:, :, start_sample:end_sample]\n\n    # Lista de nombres de los 22 canales EEG (sin EOG)\n    eeg_channel_names = [\n        'Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4',\n        'C6', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'P1', 'Pz', 'P2', 'POz'\n    ]\n\n    # Crear información para los canales EEG\n    info = mne.create_info(\n        ch_names=eeg_channel_names,\n        sfreq=fs,  # Usar la frecuencia original\n        ch_types=[\"eeg\"] * len(eeg_channel_names)  # Todos los canales son EEG\n    )\n\n    # Cargar un montaje estándar basado en el sistema 10/20\n    montage = mne.channels.make_standard_montage('standard_1020')\n    info.set_montage(montage)\n\n    # Aplicar el cálculo del laplaciano superficial \n    laplacian_X = []\n    for trial in X:\n        # Crear un objeto RawArray para cada prueba\n        raw = mne.io.RawArray(trial, info)\n        # Calcular el laplaciano superficial\n        raw = mne.preprocessing.compute_current_source_density(raw)\n        # Obtener los datos con el laplaciano aplicado\n        laplacian_X.append(raw.get_data())\n\n    # Reconvertir a un array numpy con la misma forma original\n    X = np.stack(laplacian_X)\n\n    return X, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:24:47.301368Z","iopub.execute_input":"2025-08-01T13:24:47.301972Z","iopub.status.idle":"2025-08-01T13:24:47.309411Z","shell.execute_reply.started":"2025-08-01T13:24:47.301948Z","shell.execute_reply":"2025-08-01T13:24:47.308778Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def load_BCICIV2a(db, sbj: int, mode: str, fs: float) -> tuple:\n    \"\"\"\n    Carga los datos EEG para un sujeto específico con preprocesamiento, incluyendo filtrado y laplaciano superficial.\n    Se recorta el tiempo entre los segundos 2 y 4 y solo se incluyen las clases 'mano izquierda' (1) y 'mano derecha' (2).\n    \n    Args:\n        db (Dataset_2a): Objeto del dataset.\n        sbj (int): Identificador del sujeto (1-9).\n        mode (str): 'training' o 'testing'.\n        fs (float): Frecuencia de muestreo.\n\n    Returns:\n        tuple: Datos EEG preprocesados con laplaciano superficial y recortados en el tiempo (X), etiquetas (y).\n    \"\"\"\n    # Cargar los datos del sujeto\n    db.load_subject(sbj, mode=mode)\n    X, y = db.get_data()  # Datos y etiquetas\n    X = X[:, :-3, :]  # Seleccionar solo los canales EEG (22 canales)\n    X = X * 1e6  # Convertir a microvoltios\n\n    # Aplicar filtro Notch (50 Hz)\n    notch_freq = 50.0\n    q_factor = 30.0\n    b_notch, a_notch = iirnotch(w0=notch_freq, Q=q_factor, fs=fs)\n    X = filtfilt(b_notch, a_notch, X, axis=2)\n\n    # Aplicar filtro pasa banda (0.5 - 100 Hz)\n    lowcut = 0.5\n    highcut = 100.0\n    b_band, a_band = butter(N=4, Wn=[lowcut, highcut], btype='bandpass', fs=fs)\n    X = filtfilt(b_band, a_band, X, axis=2)\n\n    # Recortar los datos entre el segundo 2 y el segundo 4\n    start_sample = int(2 * fs)  # Inicio en el segundo 2\n    end_sample = int(4 * fs)    # Fin en el segundo 4\n    X = X[:, :, start_sample:end_sample]\n\n    # Filtrar solo las clases de interés (1: mano izquierda, 2: mano derecha)\n    clases = [0, 1]\n    mask = np.isin(y, clases)\n    X = X[mask]\n    y = y[mask]\n\n    # Lista de nombres de los 22 canales EEG (sin EOG)\n    eeg_channel_names = [\n        'Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4',\n        'C6', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'P1', 'Pz', 'P2', 'POz'\n    ]\n\n    # Crear información para los canales EEG\n    info = mne.create_info(\n        ch_names=eeg_channel_names,\n        sfreq=fs,  # Usar la frecuencia original\n        ch_types=[\"eeg\"] * len(eeg_channel_names)  # Todos los canales son EEG\n    )\n\n    # Cargar un montaje estándar basado en el sistema 10/20\n    montage = mne.channels.make_standard_montage('standard_1020')\n    info.set_montage(montage)\n\n    # Aplicar el cálculo del laplaciano superficial \n    laplacian_X = []\n    for trial in X:\n        # Crear un objeto RawArray para cada prueba\n        raw = mne.io.RawArray(trial, info)\n        # Calcular el laplaciano superficial\n        raw = mne.preprocessing.compute_current_source_density(raw)\n        # Obtener los datos con el laplaciano aplicado\n        laplacian_X.append(raw.get_data())\n\n    # Reconvertir a un array numpy con la misma forma original\n    X = np.stack(laplacian_X)\n\n    return X, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:24:51.716136Z","iopub.execute_input":"2025-08-01T13:24:51.716702Z","iopub.status.idle":"2025-08-01T13:24:51.724148Z","shell.execute_reply.started":"2025-08-01T13:24:51.716678Z","shell.execute_reply":"2025-08-01T13:24:51.723380Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"@register_keras_serializable(package=\"CustomLayers\")\nclass TakensConv1D(tf.keras.layers.Layer):\n    def __init__(self, dx=5, dy=5, tau=1, mu=4, **kwargs):\n        super().__init__(**kwargs)\n        self.dx = int(dx)\n        self.dy = int(dy)\n        self.tau = int(tau)\n        self.mu = int(mu)\n        self.num_filters = self.dx + self.dy + 1\n\n    def build(self, input_shape):\n        kernel_size = self.mu + (self.dx - 1) * self.tau + 1\n        kernel_shape = (kernel_size, 1, self.num_filters)\n        kernel = tf.zeros(kernel_shape, dtype=tf.float32)\n\n        offsets_x = self.mu + tf.range(self.dx) * self.tau\n        offsets_y = tf.range(1, self.dy + 1) * self.tau\n        offset_y_t = tf.constant([0], dtype=tf.int32)\n\n        filas_x = tf.range(self.dx)\n        filas_y = self.dx + tf.range(self.dy)\n        fila_y_t = tf.constant([self.dx + self.dy])\n\n        cols_x = filas_x\n        cols_y = filas_y\n        col_y_t = fila_y_t\n\n        idx_x = tf.stack([filas_x, offsets_x, cols_x], axis=1)\n        idx_y = tf.stack([filas_y, offsets_y, cols_y], axis=1)\n        idx_yt = tf.stack([fila_y_t, offset_y_t, col_y_t], axis=1)\n\n        indices_total = tf.concat([idx_x, idx_y, idx_yt], axis=0)\n        updates = tf.ones([tf.shape(indices_total)[0]], dtype=tf.float32)\n\n        sort_order = tf.argsort(indices_total[:, 0])\n        indices_sorted = tf.gather(indices_total, sort_order)\n\n        row_for_scatter = tf.cast(indices_sorted[:, 1], tf.int32)\n        col_for_scatter = tf.cast(indices_sorted[:, 2], tf.int32)\n\n        final_indices = tf.stack([row_for_scatter,\n                                  tf.zeros_like(row_for_scatter),\n                                  col_for_scatter], axis=1)\n\n        kernel = tf.scatter_nd(final_indices, updates, kernel_shape)\n        self.kernel = kernel.numpy()[::-1]\n\n        self.conv1d = tf.keras.layers.Conv1D(\n            filters=self.num_filters,\n            kernel_size=kernel_size,\n            strides=self.tau,\n            padding=\"valid\",\n            use_bias=False\n        )\n        self.conv1d.build((None, input_shape[2], 1))\n        self.conv1d.set_weights([self.kernel])\n        self.conv1d.trainable=False\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        channels = tf.shape(inputs)[1]\n        time_steps = tf.shape(inputs)[2]\n\n        reshaped = tf.reshape(inputs, (-1, time_steps, 1))\n        conv_output = self.conv1d(reshaped)\n        new_time = tf.shape(conv_output)[1]\n\n        output = tf.reshape(conv_output, \n                            (batch_size, channels, new_time, self.num_filters))\n\n        x_sub_t_minus_mu = output[..., :self.dx]\n        y_sub_t_minus_1 = output[..., self.dx:self.dx + self.dy]\n        y_sub_t = output[..., -1:]\n\n        return x_sub_t_minus_mu, y_sub_t_minus_1, y_sub_t\n       \n\n@register_keras_serializable(package=\"CustomLayers\")\nclass KernelLayer(tf.keras.layers.Layer):\n    def __init__(self, \n                 amplitude=1.0,\n                 trainable_amplitude=False, \n                 length_scale=1.0,\n                 trainable_length_scale=False,\n                 alpha=1.0,  # Solo usado para Rational Quadratic\n                 trainable_alpha=False,\n                 kernel_type=\"gaussian\",  # \"gaussian\" o \"rational_quadratic\"\n                 **kwargs):\n        super(KernelLayer, self).__init__(**kwargs)\n\n        self.init_amplitude = amplitude\n        self.trainable_amplitude = trainable_amplitude\n        self.init_length_scale = length_scale\n        self.trainable_length_scale = trainable_length_scale\n        self.init_alpha = alpha\n        self.trainable_alpha = trainable_alpha\n        self.kernel_type = kernel_type.lower()\n\n    def build(self, input_shape):\n        self.amplitude = self.add_weight(\n            name=\"amplitude\",\n            shape=(),\n            initializer=tf.constant_initializer(self.init_amplitude),\n            trainable=self.trainable_amplitude,\n            dtype=self.dtype\n        )\n\n        self.length_scale = self.add_weight(\n            name=\"length_scale\",\n            shape=(),\n            initializer=tf.constant_initializer(self.init_length_scale),\n            trainable=self.trainable_length_scale,\n            dtype=self.dtype\n        )\n\n        if self.kernel_type == \"rational_quadratic\":\n            self.alpha = self.add_weight(\n                name=\"alpha\",\n                shape=(),\n                initializer=tf.constant_initializer(self.init_alpha),\n                trainable=self.trainable_alpha,\n                dtype=self.dtype\n            )\n\n        super(KernelLayer, self).build(input_shape)\n\n    def call(self, X):\n        if self.kernel_type == \"gaussian\":\n            kernel = tfp.math.psd_kernels.ExponentiatedQuadratic(\n                amplitude=self.amplitude,\n                length_scale=self.length_scale\n            )\n        elif self.kernel_type == \"rational_quadratic\":\n            kernel = tfp.math.psd_kernels.RationalQuadratic(\n                amplitude=self.amplitude,\n                length_scale=self.length_scale,\n                scale_mixture_rate=self.alpha\n            )\n        else:\n            raise ValueError(f\"Unsupported kernel_type: {self.kernel_type}\")\n        \n        return kernel.matrix(X, X)\n       \n@register_keras_serializable(package=\"CustomLayers\")\nclass TransferEntropyLayer(tf.keras.layers.Layer):\n    def __init__(self, alpha=2, **kwargs):\n\n        super().__init__(**kwargs)\n        self.alpha = int(alpha)\n\n    def compute_entropy(self, K_hadamard):\n        \n        trace_hadamard = tf.reduce_sum(tf.linalg.diag_part(K_hadamard), axis=-1) \n        trace_hadamard = tf.expand_dims(tf.expand_dims(trace_hadamard, axis=-1), axis=-1)\n        K_normalized = K_hadamard / trace_hadamard\n\n        K_power = tf.linalg.matmul( K_normalized , K_normalized, grad_a=True, grad_b=True ) \n        trace_power = tf.reduce_sum(tf.linalg.diag_part(K_power), axis=-1)  \n        H_alpha = (1 / (1 - self.alpha)) * tf.math.log(trace_power)\n        return H_alpha\n\n    def call(self, K_x, K_y_minus_1, K_y):\n        \n        K_x_exp = tf.expand_dims(K_x, axis=2)\n        \n        K_y_minus_1_exp = tf.expand_dims(K_y_minus_1, axis=1)\n        K_y_exp = tf.expand_dims(K_y, axis=1)\n\n        \n        H_1 = self.compute_entropy(K_y_minus_1_exp * K_x_exp)\n        H_2 = self.compute_entropy(K_y_exp * K_y_minus_1_exp * K_x_exp)\n        H_3 = self.compute_entropy(K_y_exp * K_y_minus_1_exp)\n        H_4 = self.compute_entropy(K_y_minus_1_exp)\n\n        TE = H_1 - H_2 + H_3 - H_4\n        \n        return TE\n        \n@register_keras_serializable(package=\"CustomLayers\")      \nclass RemoveDiagonalFlatten(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n    \n        super().__init__(**kwargs)\n\n    def call(self, inputs):\n        \n        shape_dyn = tf.shape(inputs)  \n        batch_size = shape_dyn[0]\n        c = tf.shape(inputs)[1]\n\n        \n        tf.debugging.assert_equal(\n            tf.shape(inputs)[1], tf.shape(inputs)[2],\n            message=\"RemoveDiagonalFlatten: la matriz de entrada no es cuadrada.\"\n        )  \n        diag_mask = tf.eye(c, dtype=inputs.dtype)  \n        inputs_no_diag = inputs * (1 - diag_mask) \n        flattened = tf.reshape(inputs_no_diag, [batch_size, -1])  \n        non_diag = tf.boolean_mask(flattened, tf.reshape(1 - diag_mask, [-1]), axis=1)\n        num_features = c * (c - 1)  \n        result = tf.reshape(non_diag, [batch_size, num_features])  \n\n        return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:24:58.805107Z","iopub.execute_input":"2025-08-01T13:24:58.805442Z","iopub.status.idle":"2025-08-01T13:24:58.826423Z","shell.execute_reply.started":"2025-08-01T13:24:58.805419Z","shell.execute_reply":"2025-08-01T13:24:58.825889Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from tensorflow.keras.metrics import Metric\n@register_keras_serializable(package=\"CustomMetrics\")\nclass Sensitivity(Metric):\n    def __init__(self, name='sensitivity', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.tp = tf.keras.metrics.TruePositives()\n        self.fn = tf.keras.metrics.FalseNegatives()\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.tp.update_state(y_true, y_pred, sample_weight)\n        self.fn.update_state(y_true, y_pred, sample_weight)\n\n    def result(self):\n        return self.tp.result() / (self.tp.result() + self.fn.result() + tf.keras.backend.epsilon())\n\n    def reset_states(self):\n        self.tp.reset_states()\n        self.fn.reset_states()\n\n    def get_config(self):\n        base_config = super().get_config()\n        return base_config\n\n\n@register_keras_serializable(package=\"CustomMetrics\")\nclass Accuracy(Metric):\n    def __init__(self, name='accuracy', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.tp = tf.keras.metrics.TruePositives()\n        self.tn = tf.keras.metrics.TrueNegatives()\n        self.fp = tf.keras.metrics.FalsePositives()\n        self.fn = tf.keras.metrics.FalseNegatives()\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.tp.update_state(y_true, y_pred, sample_weight)\n        self.tn.update_state(y_true, y_pred, sample_weight)\n        self.fp.update_state(y_true, y_pred, sample_weight)\n        self.fn.update_state(y_true, y_pred, sample_weight)\n\n    def result(self):\n        num = self.tp.result() + self.tn.result()\n        den = num + self.fp.result() + self.fn.result() + tf.keras.backend.epsilon()\n        return num / den\n\n    def reset_states(self):\n        self.tp.reset_states()\n        self.tn.reset_states()\n        self.fp.reset_states()\n        self.fn.reset_states()\n\n    def get_config(self):\n        base_config = super().get_config()\n        return base_config\n\n\n@register_keras_serializable(package=\"CustomMetrics\")\nclass F1Score(Metric):\n    def __init__(self, name='f1_score', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.tp = tf.keras.metrics.TruePositives()\n        self.fp = tf.keras.metrics.FalsePositives()\n        self.fn = tf.keras.metrics.FalseNegatives()\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.tp.update_state(y_true, y_pred, sample_weight)\n        self.fp.update_state(y_true, y_pred, sample_weight)\n        self.fn.update_state(y_true, y_pred, sample_weight)\n\n    def result(self):\n        precision = self.tp.result() / (self.tp.result() + self.fp.result() + tf.keras.backend.epsilon())\n        recall = self.tp.result() / (self.tp.result() + self.fn.result() + tf.keras.backend.epsilon())\n        return 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n\n    def reset_states(self):\n        self.tp.reset_states()\n        self.fp.reset_states()\n        self.fn.reset_states()\n\n    def get_config(self):\n        base_config = super().get_config()\n        return base_config\n\n\n@register_keras_serializable(package=\"CustomMetrics\")\nclass Specificity(Metric):\n    def __init__(self, name='specificity', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.tn = tf.keras.metrics.TrueNegatives()\n        self.fp = tf.keras.metrics.FalsePositives()\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        self.tn.update_state(y_true, y_pred, sample_weight)\n        self.fp.update_state(y_true, y_pred, sample_weight)\n\n    def result(self):\n        return self.tn.result() / (self.tn.result() + self.fp.result() + tf.keras.backend.epsilon())\n\n    def reset_states(self):\n        self.tn.reset_states()\n        self.fp.reset_states()\n\n    def get_config(self):\n        base_config = super().get_config()\n        return base_config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:25:06.711956Z","iopub.execute_input":"2025-08-01T13:25:06.712217Z","iopub.status.idle":"2025-08-01T13:25:06.726745Z","shell.execute_reply.started":"2025-08-01T13:25:06.712197Z","shell.execute_reply":"2025-08-01T13:25:06.725931Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import botocore, tensorboard, urllib3\nprint(\"botocore:\", botocore.__version__)\nprint(\"tensorboard:\", tensorboard.__version__)\nprint(\"urllib3:\", urllib3.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:25:12.180297Z","iopub.execute_input":"2025-08-01T13:25:12.180843Z","iopub.status.idle":"2025-08-01T13:25:12.187541Z","shell.execute_reply.started":"2025-08-01T13:25:12.180819Z","shell.execute_reply":"2025-08-01T13:25:12.186661Z"}},"outputs":[{"name":"stdout","text":"botocore: 1.40.0\ntensorboard: 2.20.0\nurllib3: 1.26.20\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Crear instancia del dataset\ndb = Dataset_2a('/kaggle/input/dataset-2a')\nfs = 250.0 \n\n# Cargar los datos del sujeto en modo 'training'\nX, y = load_BCICIV2a(db, sbj=4, mode='training', fs=fs)\nprint(f'tamaño de X:', X.shape)\nprint(f'tamaño de y:', y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:25:45.436125Z","iopub.execute_input":"2025-08-01T13:25:45.436412Z","iopub.status.idle":"2025-08-01T13:25:51.813232Z","shell.execute_reply.started":"2025-08-01T13:25:45.436392Z","shell.execute_reply":"2025-08-01T13:25:51.812422Z"}},"outputs":[{"name":"stdout","text":"tamaño de X: (129, 22, 500)\ntamaño de y: (129,)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"train_data, val_data, train_labels, val_labels = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=72,\n    stratify=y\n)\n\ndef create_tunable_model(dx, dy, tau, mu,\n                         kernel_type, learning_rate,\n                         kernel_size,\n                         dropout_rate): \n    input_eeg = tf.keras.Input(shape=(22, 500), name='input_eeg')\n\n    x = tf.keras.layers.DepthwiseConv1D(\n        kernel_size=kernel_size,\n        strides=1,\n        padding='valid',\n        activation='relu',\n        depth_multiplier=1,\n        data_format=\"channels_first\",\n        name='block1_depthwise_conv1d'\n    )(input_eeg)\n\n    x = tf.keras.layers.AveragePooling1D(\n        pool_size=4,\n        strides=4,\n        padding='valid',\n        data_format=\"channels_first\",\n        name='block1_avg_pooling'\n    )(x)\n\n    x = tf.keras.layers.Dropout(dropout_rate, name='block1_dropout')(x)\n\n    x = tf.keras.layers.BatchNormalization(\n        axis=1,\n        name='block1_batch_norm'\n    )(x)\n\n    # Bloque 2: TakensConv1D\n    takens = TakensConv1D(dx=dx, dy=dy, tau=tau, mu=mu, name='takens_conv1d')(x)\n    x_sub, y_minus_1, y_t = takens\n\n    # Proyecciones densas\n    x_sub = tf.keras.layers.Dense(dx, activation=None, use_bias=False, name='dense_proj_x')(x_sub)\n    y_minus_1 = tf.keras.layers.Dense(dy, activation=None, use_bias=False, name='dense_proj_y_1')(y_minus_1)\n    y_t = tf.keras.layers.Dense(1, activation=None, use_bias=False, name='dense_proj_y')(y_t)\n\n    # Kernel layers fijos\n    def fixed_kernel(name):\n        layer = KernelLayer(\n            amplitude=1.0, trainable_amplitude=False,\n            length_scale=1.0, trainable_length_scale=False,\n            alpha=1.0, trainable_alpha=False,\n            kernel_type=kernel_type,\n            name=name\n        )\n        layer.trainable = False\n        return layer\n\n    Kx = fixed_kernel('kernel_x')(x_sub)\n    Ky1 = fixed_kernel('kernel_y_minus_1')(y_minus_1)\n    Ky = fixed_kernel('kernel_y')(y_t)\n\n    # Transferencia de entropía + aplanado\n    TE = TransferEntropyLayer(alpha=2, name='transfer_entropy')(Kx, Ky1, Ky)\n    flat = RemoveDiagonalFlatten(name='remove_diag_flatten')(TE)\n\n    h = tf.keras.layers.Dense(10, activation='relu', name='dense_1')(flat)\n    h = tf.keras.layers.Dropout(dropout_rate, name='dense_dropout')(h)\n\n    out = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(h)\n\n    model = tf.keras.Model(inputs=input_eeg, outputs=out)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss='binary_crossentropy',\n        metrics=[\n            BinaryAccuracy(name='accuracy'),\n            F1Score(name='f1_score'),\n            Recall(name='sensitivity'),\n            Specificity(name='specificity')\n        ]\n    )\n    return model\n\ndef build_model(hp):\n    # Hiperparámetros a buscar\n    dx = hp.Int('dx', min_value=1, max_value=10, step=1)\n    dy = hp.Int('dy', min_value=1, max_value=10, step=1)\n    tau = hp.Int('tau', min_value=1, max_value=5, step=1)\n    mu = hp.Int('mu', min_value=0, max_value=10, step=1)\n    lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n    kernel_type = 'rational_quadratic'\n    kernel_size = hp.Int('kernel_size', min_value=3, max_value=125, step=2)\n    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.05)\n   \n\n    # Nuevo: rango buscable para kernel_size\n    kernel_size = hp.Int(\n        'kernel_size',\n        min_value=3,      # ventanas muy pequeñas (3 muestras)\n        max_value=125,    # hasta ~125 muestras (~0.25 s a 500 Hz)\n        step=2            # sólo impares\n    )\n\n    # Validación de que el embedding window quepa tras Conv+Pool\n    conv_len = 500 - kernel_size + 1\n    pool_len = (conv_len - 4) // 4 + 1\n    window   = mu + (dx - 1) * tau + 1\n    if window > pool_len:\n        # Trial inválido: modelo trivial\n        m = tf.keras.Sequential([\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(1, activation='sigmoid')\n        ])\n        m.compile('adam', 'binary_crossentropy', ['accuracy'])\n        return m\n\n    # Construcción del modelo real\n    return create_tunable_model(\n        dx, dy, tau, mu,\n        kernel_type, lr,\n        kernel_size,\n        dropout_rate\n    )\n\n# Configuración del tuner\ntuner = kt.BayesianOptimization(\n    hypermodel=build_model,\n    objective=kt.Objective(\"val_accuracy\", direction=\"max\"),\n    max_trials=70,\n    executions_per_trial=2,\n    directory=\"tuner_dir4\",\n    project_name=\"takens_te_tuning_4\"\n)\n\nstop_early = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=1\n)\n\n# Inicio de la búsqueda\ntuner.search(\n    x=train_data,\n    y=train_labels,\n    validation_data=(val_data, val_labels),\n    epochs=400,\n    callbacks=[stop_early]\n)\n\n# Extracción del mejor resultado\nbest_hp    = tuner.get_best_hyperparameters(num_trials=1)[0]\nbest_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\nbest_score = best_trial.score  # val_f1_score\n\n# Guardado en DataFrame y CSV\nimport pandas as pd\n\nbest_dict = {\n    'dx':             best_hp.get('dx'),\n    'dy':             best_hp.get('dy'),\n    'tau':            best_hp.get('tau'),\n    'mu':             best_hp.get('mu'),\n    'kernel_size':    best_hp.get('kernel_size'),\n    'learning_rate':  best_hp.get('learning_rate'),\n    'dropout_rate': best_hp.get('dropout_rate'),\n    'val_accuracy':   best_score\n}\n\ndf_best = pd.DataFrame([best_dict])\nprint(df_best)\ndf_best.to_csv('best_hyperparameters_with_score4.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:26:05.265033Z","iopub.execute_input":"2025-08-01T13:26:05.265334Z","iopub.status.idle":"2025-08-01T14:32:54.811397Z","shell.execute_reply.started":"2025-08-01T13:26:05.265313Z","shell.execute_reply":"2025-08-01T14:32:54.810889Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Trial 70 Complete [00h 01m 09s]\nval_accuracy: 0.5769231021404266\n\nBest val_accuracy So Far: 0.7115384638309479\nTotal elapsed time: 01h 06m 45s\n   dx  dy  tau  mu  kernel_size  learning_rate  dropout_rate  val_accuracy\n0   6   3    5   5           57         0.0001           0.2      0.711538\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# 1) Obtener el modelo ya entrenado con los mejores hiperparámetros\nbest_model = tuner.get_best_models(num_models=1)[0]\n\n# 2) Guardarlo en /kaggle/working\nbest_model.save(\"best_hp_model4.keras\") \n\n\n# Crear instancia del dataset\ndb = Dataset_2a('/kaggle/input/dataset-2a')\nfs = 250.0 \n\n# Cargar los datos del sujeto en modo 'training'\nX, y = load_BCICIV2a(db, sbj=4, mode='evaluation', fs=fs)\nprint(f'tamaño de X:', X.shape)\nprint(f'tamaño de X:', y.shape)\n\nresults = best_model.evaluate(X, y, return_dict=True)\n\nprint(\"Resultados evaluación:\")\nprint(f\"  Accuracy:     {results['accuracy']:.4f}\")\nprint(f\"  F1 Score:     {results['f1_score']:.4f}\")\nprint(f\"  Loss:         {results['loss']:.4f}\")\nprint(f\"  Sensitivity:  {results['sensitivity']:.4f}\")\nprint(f\"  Specificity:  {results['specificity']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:36:56.121307Z","iopub.execute_input":"2025-08-01T14:36:56.121569Z","iopub.status.idle":"2025-08-01T14:37:16.046380Z","shell.execute_reply.started":"2025-08-01T14:36:56.121552Z","shell.execute_reply":"2025-08-01T14:37:16.045824Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 24 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"name":"stdout","text":"tamaño de X: (116, 22, 500)\ntamaño de X: (116,)\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 729ms/step - accuracy: 0.4917 - f1_score: 0.5782 - loss: 0.6973 - sensitivity: 0.8062 - specificity: 0.2539\nResultados evaluación:\n  Accuracy:     0.5000\n  F1 Score:     0.5972\n  Loss:         0.6954\n  Sensitivity:  0.7544\n  Specificity:  0.2542\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# 1) Recuperar los mejores HyperParameters y Trial\nbest_hp    = tuner.get_best_hyperparameters(num_trials=1)[0]\nbest_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n\n# 2) Extraer puntuación de validación (F1)\nbest_score = best_trial.score  # corresponde a val_f1_score\n\n# 3) Construir DataFrame con todos los valores\nimport pandas as pd\n\nbest_dict = {\n    'dx':             best_hp.get('dx'),\n    'dy':             best_hp.get('dy'),\n    'tau':            best_hp.get('tau'),\n    'mu':             best_hp.get('mu'),\n    'kernel_size':    best_hp.get('kernel_size'),\n    'learning_rate':  best_hp.get('learning_rate'), \n    'val_accuracy':   best_score\n}\n\ndf_best = pd.DataFrame([best_dict])\nprint(df_best)\n\n# 4) Guardar a CSV\ndf_best.to_csv('best_hyperparameters_with_score4.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:37:29.558084Z","iopub.execute_input":"2025-08-01T14:37:29.558362Z","iopub.status.idle":"2025-08-01T14:37:29.567866Z","shell.execute_reply.started":"2025-08-01T14:37:29.558341Z","shell.execute_reply":"2025-08-01T14:37:29.567137Z"}},"outputs":[{"name":"stdout","text":"   dx  dy  tau  mu  kernel_size  learning_rate  val_accuracy\n0   6   3    5   5           57         0.0001      0.711538\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# 1) Recorremos todos los trials guardados en el tuner\nrecords = []\nfor trial in tuner.oracle.trials.values():\n    # Cada trial tiene un objeto HyperParameters donde .values es un dict de {hp_name: value}\n    hp_dict = trial.hyperparameters.values.copy()\n    # Añadimos el score (val_f1_score) de este trial\n    hp_dict['val_accuracy'] = trial.score\n    # Opcional: identifica el trial\n    hp_dict['trial_id'] = trial.trial_id\n    records.append(hp_dict)\n\n# 2) Creamos el DataFrame\ndf_all = pd.DataFrame(records)\n\n# 3) Ordenamos por la métrica descendente para ver primero los mejores\ndf_all = df_all.sort_values('val_accuracy', ascending=False).reset_index(drop=True)\n\n# 4) Mostramos y guardamos\nprint(df_all)\ndf_all.to_csv('all_trials_hyperparameters4.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:37:35.578207Z","iopub.execute_input":"2025-08-01T14:37:35.578840Z","iopub.status.idle":"2025-08-01T14:37:35.607812Z","shell.execute_reply.started":"2025-08-01T14:37:35.578816Z","shell.execute_reply":"2025-08-01T14:37:35.607035Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"    dx  dy  tau  mu  learning_rate  kernel_size  dropout_rate  val_accuracy  \\\n0    6   3    5   5         0.0001           57          0.20      0.711538   \n1    3   7    3   9         0.0010          101          0.30      0.692308   \n2    3   8    4  10         0.0001            7          0.40      0.673077   \n3    4   3    2   3         0.0010          101          0.25      0.653846   \n4    4   9    5   0         0.0001          109          0.30      0.634615   \n..  ..  ..  ...  ..            ...          ...           ...           ...   \n65   3  10    2   4         0.0001           81          0.35      0.500000   \n66   3   5    4   8         0.0010           93          0.45      0.500000   \n67  10   3    4   7         0.0100           79          0.25      0.500000   \n68   2   6    1   9         0.0001           93          0.30      0.500000   \n69   9   1    4   1         0.0001           61          0.30      0.442308   \n\n   trial_id  \n0        28  \n1        56  \n2        00  \n3        59  \n4        48  \n..      ...  \n65       60  \n66       44  \n67       68  \n68       25  \n69       35  \n\n[70 rows x 9 columns]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Validación cruzada 5 Folds ","metadata":{}},{"cell_type":"code","source":"# --- Parámetros ---\nN_SPLITS     = 5\nRANDOM_STATE = 42\nEPOCHS       = 1000\nFS           = 250.0\nMODEL_PATH   = \"/kaggle/working/best_hp_model4.keras\"\n\n# Métricas a trackear\nmetrics = ['accuracy', 'specificity', 'sensitivity', 'f1_score']\n\n# Diccionarios para acumular resultados\ntrain_eval = {m: [] for m in metrics}\nval_eval   = {m: [] for m in metrics}\ntest_eval  = {m: [] for m in metrics}\n\n# Callbacks que usamos en cada fold\ndef get_callbacks(fold):\n    return [\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            mode=\"max\",\n            patience=5,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        CSVLogger(f\"continued_training_fold{fold}.log\", append=True),\n        ModelCheckpoint(\n            filepath=f\"best_model_fold{fold}.keras\",\n            monitor=\"val_accuracy\",\n            mode=\"max\",\n            save_best_only=True,\n            verbose=1\n        ),\n    ]\n\n# --- Carga de datos ---\ndb = Dataset_2a('/kaggle/input/dataset-2a')\nX_full, y_full = load_BCICIV2a(db, sbj=5, mode='training',   fs=FS)\nX_test, y_test = load_BCICIV2a(db, sbj=5, mode='evaluation', fs=FS)\n\nprint(f\"Training set:   {X_full.shape}, {y_full.shape}\")\nprint(f\"Evaluation set: {X_test.shape},  {y_test.shape}\")\n\n# Preparamos el Stratified K-Fold\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_full, y_full), start=1):\n    print(f\"\\n=== Fold {fold}/{N_SPLITS} ===\")\n    X_train, y_train = X_full[train_idx], y_full[train_idx]\n    X_val,   y_val   = X_full[val_idx],   y_full[val_idx]\n\n    # 1) Cargar modelo con pesos y compilación original\n    model = load_model(MODEL_PATH, compile=True)\n\n    # 2) Continuar entrenamiento en este fold\n    \n    model.fit(\n        x=X_train, y=y_train,\n        validation_data=(X_val, y_val),\n        epochs=EPOCHS,\n        callbacks=get_callbacks(fold),\n        verbose=2\n    )\n\n    # 3) Cargar el mejor modelo guardado por ModelCheckpoint\n    best_model = load_model(f\"best_model_fold{fold}.keras\", compile=True)\n\n    # 4) Evaluar en train, validación y evaluación final\n    res_tr   = best_model.evaluate(X_train, y_train, return_dict=True, verbose=0)\n    res_va   = best_model.evaluate(X_val,   y_val,   return_dict=True, verbose=0)\n    res_test = best_model.evaluate(X_test,  y_test,  return_dict=True, verbose=0)\n\n    # 5) Almacenar cada métrica\n    for m in metrics:\n        train_eval[m].append( res_tr[m] )\n        val_eval[m].append(   res_va[m] )\n        test_eval[m].append(  res_test[m] )\n\n# --- Función para mostrar resumen ---\ndef summarize(eval_dict, title):\n    print(f\"\\n--- {title} ---\")\n    for m, vals in eval_dict.items():\n        mean = np.mean(vals)\n        std  = np.std(vals, ddof=1)\n        print(f\"{m:12s}: {mean:.4f} ± {std:.4f}\")\n\n# Imprimimos los promedios ± desviaciones estándar\nsummarize(train_eval, \"TRAIN   (best model por fold)\")\nsummarize(val_eval,   \"VALIDATION (best model por fold)\")\nsummarize(test_eval,  \"EVALUATION  (best model por fold)\")","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-08-01T14:45:25.278896Z","iopub.execute_input":"2025-08-01T14:45:25.279144Z","iopub.status.idle":"2025-08-01T14:47:58.944110Z","shell.execute_reply.started":"2025-08-01T14:45:25.279127Z","shell.execute_reply":"2025-08-01T14:47:58.943434Z"}},"outputs":[{"name":"stdout","text":"Training set:   (129, 22, 500), (129,)\nEvaluation set: (135, 22, 500),  (135,)\n\n=== Fold 1/5 ===\nEpoch 1/1000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 24 variables whereas the saved optimizer has 2 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: val_accuracy improved from -inf to 0.46154, saving model to best_model_fold1.keras\n4/4 - 17s - 4s/step - accuracy: 0.5146 - f1_score: 0.5833 - loss: 0.6938 - sensitivity: 0.6604 - specificity: 0.3600 - val_accuracy: 0.4615 - val_f1_score: 0.6111 - val_loss: 0.7001 - val_sensitivity: 0.8462 - val_specificity: 0.0769\nEpoch 2/1000\n\nEpoch 2: val_accuracy improved from 0.46154 to 0.53846, saving model to best_model_fold1.keras\n4/4 - 0s - 56ms/step - accuracy: 0.5631 - f1_score: 0.6281 - loss: 0.6881 - sensitivity: 0.7170 - specificity: 0.4000 - val_accuracy: 0.5385 - val_f1_score: 0.6842 - val_loss: 0.6995 - val_sensitivity: 1.0000 - val_specificity: 0.0769\nEpoch 3/1000\n\nEpoch 3: val_accuracy improved from 0.53846 to 0.57692, saving model to best_model_fold1.keras\n4/4 - 0s - 54ms/step - accuracy: 0.6117 - f1_score: 0.6610 - loss: 0.6878 - sensitivity: 0.7358 - specificity: 0.4800 - val_accuracy: 0.5769 - val_f1_score: 0.7027 - val_loss: 0.6996 - val_sensitivity: 1.0000 - val_specificity: 0.1538\nEpoch 4/1000\n\nEpoch 4: val_accuracy improved from 0.57692 to 0.61538, saving model to best_model_fold1.keras\n4/4 - 0s - 53ms/step - accuracy: 0.4951 - f1_score: 0.5873 - loss: 0.6999 - sensitivity: 0.6981 - specificity: 0.2800 - val_accuracy: 0.6154 - val_f1_score: 0.7222 - val_loss: 0.6996 - val_sensitivity: 1.0000 - val_specificity: 0.2308\nEpoch 5/1000\n\nEpoch 5: val_accuracy did not improve from 0.61538\n4/4 - 0s - 37ms/step - accuracy: 0.6117 - f1_score: 0.6491 - loss: 0.6816 - sensitivity: 0.6981 - specificity: 0.5200 - val_accuracy: 0.6154 - val_f1_score: 0.7222 - val_loss: 0.6991 - val_sensitivity: 1.0000 - val_specificity: 0.2308\nEpoch 6/1000\n\nEpoch 6: val_accuracy did not improve from 0.61538\n4/4 - 0s - 37ms/step - accuracy: 0.5146 - f1_score: 0.5833 - loss: 0.6904 - sensitivity: 0.6604 - specificity: 0.3600 - val_accuracy: 0.5769 - val_f1_score: 0.6857 - val_loss: 0.6984 - val_sensitivity: 0.9231 - val_specificity: 0.2308\nEpoch 7/1000\n\nEpoch 7: val_accuracy did not improve from 0.61538\n4/4 - 0s - 36ms/step - accuracy: 0.5534 - f1_score: 0.6102 - loss: 0.6893 - sensitivity: 0.6792 - specificity: 0.4200 - val_accuracy: 0.4615 - val_f1_score: 0.5333 - val_loss: 0.6973 - val_sensitivity: 0.6154 - val_specificity: 0.3077\nEpoch 8/1000\n\nEpoch 8: val_accuracy did not improve from 0.61538\n4/4 - 0s - 36ms/step - accuracy: 0.4854 - f1_score: 0.5310 - loss: 0.6910 - sensitivity: 0.5660 - specificity: 0.4000 - val_accuracy: 0.3846 - val_f1_score: 0.4286 - val_loss: 0.6968 - val_sensitivity: 0.4615 - val_specificity: 0.3077\nEpoch 9/1000\n\nEpoch 9: val_accuracy did not improve from 0.61538\n4/4 - 0s - 37ms/step - accuracy: 0.4369 - f1_score: 0.4727 - loss: 0.6977 - sensitivity: 0.4906 - specificity: 0.3800 - val_accuracy: 0.4615 - val_f1_score: 0.4615 - val_loss: 0.6965 - val_sensitivity: 0.4615 - val_specificity: 0.4615\nEpoch 9: early stopping\nRestoring model weights from the end of the best epoch: 4.\n\n=== Fold 2/5 ===\nEpoch 1/1000\n\nEpoch 1: val_accuracy improved from -inf to 0.46154, saving model to best_model_fold2.keras\n4/4 - 17s - 4s/step - accuracy: 0.4563 - f1_score: 0.5254 - loss: 0.6980 - sensitivity: 0.5849 - specificity: 0.3200 - val_accuracy: 0.4615 - val_f1_score: 0.6111 - val_loss: 0.6936 - val_sensitivity: 0.8462 - val_specificity: 0.0769\nEpoch 2/1000\n\nEpoch 2: val_accuracy did not improve from 0.46154\n4/4 - 0s - 39ms/step - accuracy: 0.5437 - f1_score: 0.6240 - loss: 0.6916 - sensitivity: 0.7358 - specificity: 0.3400 - val_accuracy: 0.4615 - val_f1_score: 0.6111 - val_loss: 0.6944 - val_sensitivity: 0.8462 - val_specificity: 0.0769\nEpoch 3/1000\n\nEpoch 3: val_accuracy improved from 0.46154 to 0.50000, saving model to best_model_fold2.keras\n4/4 - 0s - 54ms/step - accuracy: 0.4466 - f1_score: 0.5440 - loss: 0.6941 - sensitivity: 0.6415 - specificity: 0.2400 - val_accuracy: 0.5000 - val_f1_score: 0.6486 - val_loss: 0.6958 - val_sensitivity: 0.9231 - val_specificity: 0.0769\nEpoch 4/1000\n\nEpoch 4: val_accuracy did not improve from 0.50000\n4/4 - 0s - 38ms/step - accuracy: 0.5340 - f1_score: 0.6190 - loss: 0.6902 - sensitivity: 0.7358 - specificity: 0.3200 - val_accuracy: 0.5000 - val_f1_score: 0.6486 - val_loss: 0.6965 - val_sensitivity: 0.9231 - val_specificity: 0.0769\nEpoch 5/1000\n\nEpoch 5: val_accuracy did not improve from 0.50000\n4/4 - 0s - 37ms/step - accuracy: 0.5825 - f1_score: 0.6504 - loss: 0.6879 - sensitivity: 0.7547 - specificity: 0.4000 - val_accuracy: 0.5000 - val_f1_score: 0.6486 - val_loss: 0.6973 - val_sensitivity: 0.9231 - val_specificity: 0.0769\nEpoch 6/1000\n\nEpoch 6: val_accuracy did not improve from 0.50000\n4/4 - 0s - 38ms/step - accuracy: 0.5243 - f1_score: 0.6202 - loss: 0.6998 - sensitivity: 0.7547 - specificity: 0.2800 - val_accuracy: 0.5000 - val_f1_score: 0.6486 - val_loss: 0.6978 - val_sensitivity: 0.9231 - val_specificity: 0.0769\nEpoch 7/1000\n\nEpoch 7: val_accuracy did not improve from 0.50000\n4/4 - 0s - 39ms/step - accuracy: 0.5340 - f1_score: 0.6250 - loss: 0.6924 - sensitivity: 0.7547 - specificity: 0.3000 - val_accuracy: 0.5000 - val_f1_score: 0.6486 - val_loss: 0.6981 - val_sensitivity: 0.9231 - val_specificity: 0.0769\nEpoch 8/1000\n\nEpoch 8: val_accuracy did not improve from 0.50000\n4/4 - 0s - 37ms/step - accuracy: 0.5534 - f1_score: 0.6567 - loss: 0.6875 - sensitivity: 0.8302 - specificity: 0.2600 - val_accuracy: 0.5000 - val_f1_score: 0.6486 - val_loss: 0.6984 - val_sensitivity: 0.9231 - val_specificity: 0.0769\nEpoch 8: early stopping\nRestoring model weights from the end of the best epoch: 3.\n\n=== Fold 3/5 ===\nEpoch 1/1000\n\nEpoch 1: val_accuracy improved from -inf to 0.61538, saving model to best_model_fold3.keras\n4/4 - 23s - 6s/step - accuracy: 0.4854 - f1_score: 0.5391 - loss: 0.6899 - sensitivity: 0.5849 - specificity: 0.3800 - val_accuracy: 0.6154 - val_f1_score: 0.7059 - val_loss: 0.6795 - val_sensitivity: 0.9231 - val_specificity: 0.3077\nEpoch 2/1000\n\nEpoch 2: val_accuracy improved from 0.61538 to 0.65385, saving model to best_model_fold3.keras\n4/4 - 0s - 55ms/step - accuracy: 0.5437 - f1_score: 0.5766 - loss: 0.6915 - sensitivity: 0.6038 - specificity: 0.4800 - val_accuracy: 0.6538 - val_f1_score: 0.7273 - val_loss: 0.6806 - val_sensitivity: 0.9231 - val_specificity: 0.3846\nEpoch 3/1000\n\nEpoch 3: val_accuracy did not improve from 0.65385\n4/4 - 0s - 38ms/step - accuracy: 0.5631 - f1_score: 0.6087 - loss: 0.6905 - sensitivity: 0.6604 - specificity: 0.4600 - val_accuracy: 0.6538 - val_f1_score: 0.7429 - val_loss: 0.6822 - val_sensitivity: 1.0000 - val_specificity: 0.3077\nEpoch 4/1000\n\nEpoch 4: val_accuracy did not improve from 0.65385\n4/4 - 0s - 38ms/step - accuracy: 0.5340 - f1_score: 0.5789 - loss: 0.6882 - sensitivity: 0.6226 - specificity: 0.4400 - val_accuracy: 0.6154 - val_f1_score: 0.7222 - val_loss: 0.6831 - val_sensitivity: 1.0000 - val_specificity: 0.2308\nEpoch 5/1000\n\nEpoch 5: val_accuracy did not improve from 0.65385\n4/4 - 0s - 37ms/step - accuracy: 0.5825 - f1_score: 0.6055 - loss: 0.6846 - sensitivity: 0.6226 - specificity: 0.5400 - val_accuracy: 0.6538 - val_f1_score: 0.7429 - val_loss: 0.6839 - val_sensitivity: 1.0000 - val_specificity: 0.3077\nEpoch 6/1000\n\nEpoch 6: val_accuracy did not improve from 0.65385\n4/4 - 0s - 36ms/step - accuracy: 0.5437 - f1_score: 0.5607 - loss: 0.6859 - sensitivity: 0.5660 - specificity: 0.5200 - val_accuracy: 0.6538 - val_f1_score: 0.7273 - val_loss: 0.6846 - val_sensitivity: 0.9231 - val_specificity: 0.3846\nEpoch 7/1000\n\nEpoch 7: val_accuracy did not improve from 0.65385\n4/4 - 0s - 37ms/step - accuracy: 0.5631 - f1_score: 0.5946 - loss: 0.6911 - sensitivity: 0.6226 - specificity: 0.5000 - val_accuracy: 0.5769 - val_f1_score: 0.6452 - val_loss: 0.6851 - val_sensitivity: 0.7692 - val_specificity: 0.3846\nEpoch 7: early stopping\nRestoring model weights from the end of the best epoch: 2.\n\n=== Fold 4/5 ===\nEpoch 1/1000\n\nEpoch 1: val_accuracy improved from -inf to 0.53846, saving model to best_model_fold4.keras\n4/4 - 18s - 4s/step - accuracy: 0.4757 - f1_score: 0.5345 - loss: 0.7036 - sensitivity: 0.5962 - specificity: 0.3529 - val_accuracy: 0.5385 - val_f1_score: 0.6842 - val_loss: 0.7027 - val_sensitivity: 0.9286 - val_specificity: 0.0833\nEpoch 2/1000\n\nEpoch 2: val_accuracy improved from 0.53846 to 0.57692, saving model to best_model_fold4.keras\n4/4 - 0s - 56ms/step - accuracy: 0.4951 - f1_score: 0.5517 - loss: 0.6948 - sensitivity: 0.6154 - specificity: 0.3725 - val_accuracy: 0.5769 - val_f1_score: 0.7179 - val_loss: 0.6961 - val_sensitivity: 1.0000 - val_specificity: 0.0833\nEpoch 3/1000\n\nEpoch 3: val_accuracy did not improve from 0.57692\n4/4 - 0s - 40ms/step - accuracy: 0.5146 - f1_score: 0.5833 - loss: 0.6989 - sensitivity: 0.6731 - specificity: 0.3529 - val_accuracy: 0.5769 - val_f1_score: 0.7179 - val_loss: 0.6911 - val_sensitivity: 1.0000 - val_specificity: 0.0833\nEpoch 4/1000\n\nEpoch 4: val_accuracy did not improve from 0.57692\n4/4 - 0s - 38ms/step - accuracy: 0.4951 - f1_score: 0.6000 - loss: 0.6942 - sensitivity: 0.7500 - specificity: 0.2353 - val_accuracy: 0.5769 - val_f1_score: 0.7179 - val_loss: 0.6880 - val_sensitivity: 1.0000 - val_specificity: 0.0833\nEpoch 5/1000\n\nEpoch 5: val_accuracy did not improve from 0.57692\n4/4 - 0s - 38ms/step - accuracy: 0.5534 - f1_score: 0.6406 - loss: 0.6874 - sensitivity: 0.7885 - specificity: 0.3137 - val_accuracy: 0.5769 - val_f1_score: 0.7179 - val_loss: 0.6860 - val_sensitivity: 1.0000 - val_specificity: 0.0833\nEpoch 6/1000\n\nEpoch 6: val_accuracy did not improve from 0.57692\n4/4 - 0s - 39ms/step - accuracy: 0.4757 - f1_score: 0.5781 - loss: 0.6970 - sensitivity: 0.7115 - specificity: 0.2353 - val_accuracy: 0.5385 - val_f1_score: 0.7000 - val_loss: 0.6846 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00\nEpoch 7/1000\n\nEpoch 7: val_accuracy did not improve from 0.57692\n4/4 - 0s - 38ms/step - accuracy: 0.5049 - f1_score: 0.6222 - loss: 0.6902 - sensitivity: 0.8077 - specificity: 0.1961 - val_accuracy: 0.5385 - val_f1_score: 0.7000 - val_loss: 0.6834 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00\nEpoch 7: early stopping\nRestoring model weights from the end of the best epoch: 2.\n\n=== Fold 5/5 ===\nEpoch 1/1000\n\nEpoch 1: val_accuracy improved from -inf to 0.56000, saving model to best_model_fold5.keras\n4/4 - 18s - 4s/step - accuracy: 0.5577 - f1_score: 0.6290 - loss: 0.6898 - sensitivity: 0.7358 - specificity: 0.3725 - val_accuracy: 0.5600 - val_f1_score: 0.6452 - val_loss: 0.6832 - val_sensitivity: 0.7692 - val_specificity: 0.3333\nEpoch 2/1000\n\nEpoch 2: val_accuracy improved from 0.56000 to 0.64000, saving model to best_model_fold5.keras\n4/4 - 0s - 57ms/step - accuracy: 0.4712 - f1_score: 0.4954 - loss: 0.6948 - sensitivity: 0.5094 - specificity: 0.4314 - val_accuracy: 0.6400 - val_f1_score: 0.6897 - val_loss: 0.6852 - val_sensitivity: 0.7692 - val_specificity: 0.5000\nEpoch 3/1000\n\nEpoch 3: val_accuracy did not improve from 0.64000\n4/4 - 0s - 38ms/step - accuracy: 0.6154 - f1_score: 0.6226 - loss: 0.6864 - sensitivity: 0.6226 - specificity: 0.6078 - val_accuracy: 0.4800 - val_f1_score: 0.4800 - val_loss: 0.6877 - val_sensitivity: 0.4615 - val_specificity: 0.5000\nEpoch 4/1000\n\nEpoch 4: val_accuracy did not improve from 0.64000\n4/4 - 0s - 38ms/step - accuracy: 0.5192 - f1_score: 0.5370 - loss: 0.6919 - sensitivity: 0.5472 - specificity: 0.4902 - val_accuracy: 0.4400 - val_f1_score: 0.3636 - val_loss: 0.6899 - val_sensitivity: 0.3077 - val_specificity: 0.5833\nEpoch 5/1000\n\nEpoch 5: val_accuracy did not improve from 0.64000\n4/4 - 0s - 38ms/step - accuracy: 0.6058 - f1_score: 0.6019 - loss: 0.6856 - sensitivity: 0.5849 - specificity: 0.6275 - val_accuracy: 0.4400 - val_f1_score: 0.3000 - val_loss: 0.6916 - val_sensitivity: 0.2308 - val_specificity: 0.6667\nEpoch 6/1000\n\nEpoch 6: val_accuracy did not improve from 0.64000\n4/4 - 0s - 36ms/step - accuracy: 0.5385 - f1_score: 0.5472 - loss: 0.6901 - sensitivity: 0.5472 - specificity: 0.5294 - val_accuracy: 0.4800 - val_f1_score: 0.2353 - val_loss: 0.6938 - val_sensitivity: 0.1538 - val_specificity: 0.8333\nEpoch 7/1000\n\nEpoch 7: val_accuracy did not improve from 0.64000\n4/4 - 0s - 36ms/step - accuracy: 0.5865 - f1_score: 0.5567 - loss: 0.6824 - sensitivity: 0.5094 - specificity: 0.6667 - val_accuracy: 0.4800 - val_f1_score: 0.2353 - val_loss: 0.6954 - val_sensitivity: 0.1538 - val_specificity: 0.8333\nEpoch 7: early stopping\nRestoring model weights from the end of the best epoch: 2.\n\n--- TRAIN   (best model por fold) ---\naccuracy    : 0.5446 ± 0.0267\nspecificity : 0.2060 ± 0.1475\nsensitivity : 0.8677 ± 0.1385\nf1_score    : 0.6577 ± 0.0452\n\n--- VALIDATION (best model por fold) ---\naccuracy    : 0.5972 ± 0.0617\nspecificity : 0.2551 ± 0.1861\nsensitivity : 0.9231 ± 0.0942\nf1_score    : 0.7011 ± 0.0328\n\n--- EVALUATION  (best model por fold) ---\naccuracy    : 0.5126 ± 0.0199\nspecificity : 0.1514 ± 0.1424\nsensitivity : 0.9015 ± 0.1139\nf1_score    : 0.6387 ± 0.0233\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"FS = 250.0 \ndb = Dataset_2a('/kaggle/input/dataset-2a')\nX_full, y_full = load_BCICIV2a(db, sbj=4, mode='training',   fs=FS)\nX_test, y_test = load_BCICIV2a(db, sbj=4, mode='evaluation', fs=FS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:43:44.818136Z","iopub.execute_input":"2025-08-01T14:43:44.818745Z","iopub.status.idle":"2025-08-01T14:43:54.648684Z","shell.execute_reply.started":"2025-08-01T14:43:44.818722Z","shell.execute_reply":"2025-08-01T14:43:54.647869Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"##### import numpy as np\nfrom keras.models import load_model  # o tensorflow.keras\n\nmodel_paths = [\n    \"/kaggle/working/best_model_fold1.keras\",\n    \"/kaggle/working/best_model_fold2.keras\",\n    \"/kaggle/working/best_model_fold3.keras\",\n    \"/kaggle/working/best_model_fold4.keras\",\n    \"/kaggle/working/best_model_fold5.keras\",\n    \n]\n\n# Vamos a recoger también las métricas por fold\ntrain_metrics = {}\ntest_metrics  = {}\n\nprint(\"## Accuracy por fold ##\")\nfor i, path in enumerate(model_paths, start=1):\n    model = load_model(path)\n    tr = model.evaluate(X_full, y_full, return_dict=True, verbose=0)\n    te = model.evaluate(X_test, y_test, return_dict=True, verbose=0)\n\n    # Imprimimos justo aquí la accuracy de este fold\n    print(f\"Fold {i}: Train ACC = {tr['accuracy']:.4f},  Test ACC = {te['accuracy']:.4f}\")\n\n    # Y seguimos acumulando para el resumen\n    for key, val in tr.items():\n        train_metrics.setdefault(key, []).append(val)\n    for key, val in te.items():\n        test_metrics.setdefault(key, []).append(val)\n\n# Luego, si quieres, mantienes tu agregado:\nprint(\"\\n--- Resultados agregados sobre 5 folds ---\")\nfor metric in train_metrics:\n    t_arr = np.array(train_metrics[metric])\n    e_arr = np.array(test_metrics[metric])\n    print(f\"{metric:12s} | Train: {t_arr.mean():.4f} ± {t_arr.std():.4f}  |  Test: {e_arr.mean():.4f} ± {e_arr.std():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:55:32.264340Z","iopub.execute_input":"2025-08-01T14:55:32.264723Z"}},"outputs":[{"name":"stdout","text":"## Accuracy por fold ##\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Std Artículo","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Datos de validación (Val. Acc) para cada sujeto [9, 8, 3, 7, 1, 5, 6, 4, 2]\nval_accuracy = np.array([0.991, 0.985, 0.942, 0.917, 0.899, 0.884, 0.726, 0.723, 0.677])\nstd_val_acc  = np.array([0.019, 0.020, 0.060, 0.069, 0.083, 0.051, 0.084, 0.137, 0.084])\n\n# Datos de accuracy para cada sujeto\naccuracy = np.array([0.869, 0.887, 0.885, 0.611, 0.759, 0.613, 0.565, 0.599, 0.621])\nstd_acc  = np.array([0.007, 0.023, 0.008, 0.013, 0.010, 0.017, 0.034, 0.049, 0.034])\n\n# Datos de F1-score para cada sujeto\nf1_score = np.array([0.877, 0.886, 0.892, 0.603, 0.758, 0.659, 0.553, 0.646, 0.618])\nstd_f1    = np.array([0.005, 0.024, 0.007, 0.028, 0.011, 0.012, 0.049, 0.026, 0.056])\n\n# Datos de sensitivity para cada sujeto\nsensitivity = np.array([0.932, 0.868, 0.929, 0.600, 0.760, 0.775, 0.535, 0.744, 0.625])\nstd_sen     = np.array([0.007, 0.029, 0.006, 0.057, 0.019, 0.050, 0.079, 0.066, 0.117])\n\n# Datos de specificity para cada sujeto\nspecificity = np.array([0.806, 0.906, 0.839, 0.623, 0.758, 0.463, 0.596, 0.458, 0.6169])\nstd_spe     = np.array([0.018, 0.026, 0.017, 0.038, 0.019, 0.070, 0.079, 0.137, 0.133])\n\n# Número de sujetos\nn = val_accuracy.size\n\ndef mean_and_avg_err(values, errs):\n    \"\"\"\n    Calcula la media de `values` y la incertidumbre del promedio\n    asumiendo errores independientes:\n      sigma_prom = sqrt(sum(err_i^2)) / n\n    \"\"\"\n    mean_val = np.mean(values)\n    err_avg  = np.sqrt(np.sum(errs**2)) / n\n    return mean_val, err_avg\n\n# Calcular media e incertidumbre para cada métrica\nm_val,    e_val    = mean_and_avg_err(val_accuracy,  std_val_acc)\nm_acc,    e_acc    = mean_and_avg_err(accuracy,      std_acc)\nm_f1,     e_f1     = mean_and_avg_err(f1_score,      std_f1)\nm_sens,   e_sens   = mean_and_avg_err(sensitivity,   std_sen)\nm_spec,   e_spec   = mean_and_avg_err(specificity,   std_spe)\n\n# Mostrar resultados\nprint(f\"Validation ACC Promedio: {m_val:.4f} ± {e_val:.4f}\")\nprint(f\"Accuracy Promedio:       {m_acc:.4f} ± {e_acc:.4f}\")\nprint(f\"F1 Score Promedio:       {m_f1:.4f} ± {e_f1:.4f}\")\nprint(f\"Sensitivity Promedio:    {m_sens:.4f} ± {e_sens:.4f}\")\nprint(f\"Specificity Promedio:    {m_spec:.4f} ± {e_spec:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T17:25:22.203068Z","iopub.execute_input":"2025-07-30T17:25:22.203865Z","iopub.status.idle":"2025-07-30T17:25:22.214804Z","shell.execute_reply.started":"2025-07-30T17:25:22.203817Z","shell.execute_reply":"2025-07-30T17:25:22.213693Z"}},"outputs":[{"name":"stdout","text":"Validation ACC Promedio: 0.8604 ± 0.0252\nAccuracy Promedio:       0.7121 ± 0.0085\nF1 Score Promedio:       0.7213 ± 0.0099\nSensitivity Promedio:    0.7520 ± 0.0197\nSpecificity Promedio:    0.6740 ± 0.0250\n","output_type":"stream"}],"execution_count":33}]}